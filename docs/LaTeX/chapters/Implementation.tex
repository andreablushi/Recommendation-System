\section{Implementation}
A recommendation system was implemented using a decision tree classifier to analyze production event logs.
The implementation was achieved in Python, leveraging libraries such as \textit{PM4Py} for process mining tasks and \textit{scikit-learn} for machine learning functionalities.
The execution flow of our implementation resides within the \textit{notebook.ipynb} file, which calls all the designed utility functions.

\subsection{Encoding the Event Log}
We began by importing the event log file, which is in \textit{XES} format, using the \textit{PM4Py} library. 
We then converted this file into a \textit{PM4Py} event log object suitable for subsequent process mining tasks.
\\\\
A pruned copy of the event log was created to consider only the prefixes of each trace.
To achieve this, each trace in the log was truncated to \textit{prefix\_length} events.
Since the partial ongoing trace in a real-world scenario is incomplete, we trained the classifier 
exclusively on the prefixes of the traces.
\\\\
Next, we extracted the unique activity names from the complete event log to serve as columns (features) for the encoding step.
In addition to these, we included the trace identifier, the prefix length, and the ground truth label, 
which indicates whether the trace outcome was positive or negative.
\\\\
To prepare this data for training a decision tree classifier, the pruned events were encoded using a \textit{Boolean encoding} scheme.
This encoding involved creating binary features for each unique activity name. 
For each trace, if an activity was present within the trace, the corresponding feature in that row was set to true (1); otherwise, it was set to false (0).
This approach allows for the creation of a feature set with a fixed number of columns, eliminating the need to pad traces to a uniform length.
Upon completion, we obtained a \textit{DataFrame} in which each row represented a trace, while the feature columns indicated the presence or absence of specific activities within that trace.

\subsection{Training the Decision Tree Classifier}
Following the encoding of the pruned event log, a decision tree classifier was trained using the \textit{scikit-learn} library.  
We separated the encoded log into features $(\boldsymbol{X})$, which were derived by removing the trace identifier and label columns, and labels $(\boldsymbol{y})$, which represented the ground truth extracted directly from the log.  
The dataset had been pre-partitioned in the original \textit{XES} file into distinct training and testing sets; these partitions were used to train and evaluate the classifier, respectively.
\\\\
After training the classifier, it was possible to visualize the learned decision rules by plotting the decision tree using built-in \textit{scikit-learn} functionality.  

\subsection{Optimizing the Classifier Hyperparameters}
To enhance the performance of our decision tree classifier, we employed the \textit{Hyperopt} library to perform hyperparameter optimization.
This tool systematically explores a defined search space, iteratively testing combinations of critical hyperparameters—such as \textit{max\_depth}, \textit{max\_features}, and the split criterion (e.g., \textit{Gini} or \textit{entropy})—to identify the optimal configuration.
At every iteration, the model was trained and evaluated using the $F_1$-score as the primary performance metric. 
Only the configuration that resulted in the highest $F_1$-score was retained for the final model.
The \textit{Hyperopt} library also allowed us to define a search space and to specify the maximum number of iterations to perform.

In order to achieve reproducible results, a fixed random seed was set for all stochastic processes involved in the optimization and training phases.

\subsection{Generating Predictions}
After training, the optimized classifier was used to generate predictions on the test set. 
The predicted labels were obtained by applying the trained model to the feature set derived from the encoded test log. 
Similar to the training phase, the features $(\boldsymbol{X}_{test})$ were prepared by removing the trace identifier and ground truth label columns. 
The classifier's performance was subsequently evaluated using standard scikit-learn metrics, including \textit{Accuracy}, \textit{Precision}, \textit{Recall}, and the \textit{F1-score}.

\subsection{Extracting Recommendations}
Given the interpretability inherent in the decision tree structure, the model can provide proactive recommendations specifically for traces predicted to result in a negative outcome.
The process for generating these recommendations involved three main steps:
\begin{enumerate}
    \item \textbf{Extracting Positive Paths}: identify all distinct paths in the decision tree that lead to a leaf node associated with a positive outcome.
    \item \textbf{Filtering Compliant Paths}: for a given negatively-predicted trace prefix, filter the positive paths to find those that are compliant 
    (i.e., whose conditions are not violated by the activities already present in the trace).
    \item \textbf{Generating Recommendations}: from the compliant positive paths, select the one with the highest confidence score. Determine the set of 
    activities required to complete that path.
\end{enumerate}
Note that the function signature differs from the originally requested one.
In particular, the \textit{class\_values} parameter was removed to simplify the implementation, since our setting is restricted to binary classification only.

\paragraph{Extracting Positive Paths from the Decision Tree:}
To extract all positive paths from the decision tree, we traversed the tree structure starting from the root node using a \textit{Depth-first search} (DFS) approach.
At each decision node in the path, we recorded the feature, the operator (e.g., $\le$, $>$), and the threshold that leads to the next node.
\\Upon reaching a leaf node, if the node's label corresponded to a positive outcome, the entire path, along with its associated confidence score, was stored as a positive path.
\\The confidence score was a weighted measure based on the number of samples reaching each node.
This was crucial to avoid generating recommendations from outlier paths that were not representative of the overall dataset.

\begin{algorithm}[H]
    \label{alg:get_positive_paths}
    \caption{Get Positive Paths}
    \begin{algorithmic}[1]
    \Function{GetPositivePaths}{node, current\_path, positive\_paths}
        \If{node \textbf{is} leaf}
            \If{node.label == positive}
                \State Add (current\_path, node.confidence) to positive\_paths
            \EndIf
        \Else
            \State Append (node.feature, $\le$, node.value) to current\_path
            \State Call \textbf{GetPositivePaths}(node.left, current\_path, positive\_paths)
            \State Remove last element from current\_path
            \\
            \State Append (node.feature, $>$, node.value) to current\_path
            \State Call \textbf{GetPositivePaths}(node.right, current\_path, positive\_paths)
            
            \State Remove last element from current\_path
        \EndIf
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\paragraph{Filtering compliant paths:}
After all positive paths were extracted, for each prefix trace predicted as negative, we filtered the set to retain only the \textit{compliant} paths. 
A path is defined as compliant if and only if none of the feature conditions along the path are violated by the set of activities currently present in the prefix trace. 
Among all compliant paths, the one with the highest confidence score was selected. In case of a tie in confidence, the path with the shortest length was chosen as the tie-breaker.

\paragraph{Generate Recommendations:}
Once the most suitable positive compliant path was identified, recommendations were generated by determining the activities that needed to be added to the current trace to follow the selected path.
For each test set prefix, the recommendation set is defined as:
\begin{itemize}
    \item The \textit{None} value if the trace was already positively predicted;
    \item The empty set if the prefix was predicted as positive or if no compliant positive path was found;
    \item The set of activities corresponding to the conditions in the selected positive path that were not satisfied by the current prefix trace otherwise.
\end{itemize}

\subsection{Evaluation of Recommendations}
To evaluate the quality of the generated recommendations, we compared them against the actual full traces in the test set, where the encoded test set served as the ground truth.
\\\\The features of each full trace were matched against the recommendations generated for the corresponding prefixes.
If a recommendation was defined as \textit{None}, it was skipped, as this indicates that no action was necessary because the trace was already predicted to have a positive outcome.
Conversely, if a recommendation was an empty set, it was considered as not followed, since the system was unable to suggest any corrective action for that negative prediction.
A recommendation was considered followed if and only if all the associated boolean conditions were satisfied in the full trace.
Each condition specifies whether a given activity should be present or absent; if any single condition was violated, the recommendation was marked as not followed.
\\\\The recommendation outcome (followed or not followed) was then compared with the ground truth label of the trace to derive an approximated confusion matrix defined as follows:
\begin{itemize}
    \item \textbf{True Positives}: the recommendation was followed in the actual trace and the ground truth outcome is positive.
    \item \textbf{False Positives}: the recommendation was followed in the actual trace but the ground truth outcome is negative.
    \item \textbf{True Negatives}: the recommendation was not followed in the actual trace and the ground truth outcome is negative.
    \item \textbf{False Negatives}: the recommendation was not followed in the actual trace but the ground truth outcome is positive.
\end{itemize}

Based on this confusion matrix, we computed standard evaluation metrics, such as accuracy, precision, recall, and F1-score, to assess the effectiveness of the generated recommendations.
