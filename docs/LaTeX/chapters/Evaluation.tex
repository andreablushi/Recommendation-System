\section{Evaluation}
This section presents the evaluation results for both $\textit{prefix\_length}=5$ and $\textit{prefix\_length}=11$ of the decision tree classifier and the recommendation system built on top of it.

\subsection{Hyperparameters}
Tables~\ref{tab:hyperparameters_5} and~\ref{tab:hyperparameters_11} report the optimized hyperparameters obtained through \textit{Hyperopt} for both prefix lengths. 
\\\\
For $\textit{prefix\_length}=5$, the optimizer selects a shallow tree with a maximum depth of $3$ and a small number of features considered at each split.
This choice limits overfitting in a setting where only partial information about the trace is available, while still allowing the model to exploit the most informative activities.
The use of the entropy criterion leads to more balanced and interpretable decision paths.
\\\\
For $\textit{prefix\_length}=11$, the optimal configuration still constrains the maximum depth to $3$, confirming that deeper trees do not provide additional benefits even when more information is available.
However, the maximum number of features increases, indicating that longer prefixes allow the model to effectively leverage a richer set of activities.
In this case, the Gini criterion is selected, suggesting that it better captures class separation when more contextual information is present.

\begin{table}[ht]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Hyperparameter} & \textbf{Value} \\
            \hline
            Criterion & Entropy \\
            Max Depth & 3 \\
            Max Features & 2 \\
            \hline
        \end{tabular}
        \caption{Optimized hyperparameters for the decision tree classifier for $\textit{prefix\_length}=5$.}
        \label{tab:hyperparameters_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Hyperparameter} & \textbf{Value} \\
            \hline
            Criterion & Gini \\
            Max Depth & 3 \\
            Max Features & 3 \\
            \hline
        \end{tabular}
        \caption{Optimized hyperparameters for the decision tree classifier for $\textit{prefix\_length}=11$.}
        \label{tab:hyperparameters_11}
    \end{minipage}
\end{table}


\subsection{Decision Tree Structure}
Figures~\ref{fig:decision_tree_5} and~\ref{fig:decision_tree_11} illustrate the structure of the trained decision trees for the two prefix lengths.
Leaf nodes are color-coded (blue for positive outcomes, orange for negative outcomes) to indicate prediction distributions.
The controlled maximum depth and limited number of features ensure interpretability, preventing excessive branching.
The tree structures provide valuable insights into which activities are most discriminative for predicting case outcomes, making the model transparent.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../media/decision_tree_5.png}
    \caption{Visualization of the trained decision tree classifier for $\textit{prefix\_length}=5$. Each node represents a decision based on the presence or absence of specific activities, leading to recommendations at the leaf nodes.}
    \label{fig:decision_tree_5}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../media/decision_tree_11.png}
    \caption{Visualization of the trained decision tree classifier for $\textit{prefix\_length}=11$.}
    \label{fig:decision_tree_11}
\end{figure}

\subsubsection{Evaluation Metrics}
The predictive performance of the decision tree classifiers is summarized in Tables~\ref{tab:evaluation_metrics_5} and~\ref{tab:evaluation_metrics_11}.
\\\\
For $\textit{prefix\_length}=5$, the model exhibits moderately balanced performance, capturing meaningful patterns despite limited information in the trace prefixes.
Increasing the prefix length to $\textit{prefix\_length}=11$ provides additional contextual information, enabling the model to make more confident positive predictions.
These results confirm that increasing the prefix length generally improves outcome prediction, as more activities contribute to defining the process trajectory.
Confusion matrices in Figures~\ref{fig:confusion_matrix_5} and~\ref{fig:confusion_matrix_11} provide further insight into the distribution of predictions and misclassifications.
\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../media/confusion_matrix_5.png}
        \caption{Confusion matrix of predictions in the test set ($\textit{prefix\_length}=5$).}
        \label{fig:confusion_matrix_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../media/confusion_matrix_11.png}
        \caption{Confusion matrix of predictions in the test set ($\textit{prefix\_length}=11$).}
        \label{fig:confusion_matrix_11}
    \end{minipage}
\end{figure}

\begin{table}[H]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Metric} & \textbf{Value} \\
            \hline
            Accuracy & 69.77\% \\
            Precision & 64.84\% \\
            Recall & 69.77\% \\
            F1-Score & 66.42\% \\
            \hline
        \end{tabular}
        \caption{Evaluation metrics for the decision tree classifier on the test set for $\textit{prefix\_length}=5$.}
        \label{tab:evaluation_metrics_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Metric} & \textbf{Value} \\
            \hline
            Accuracy & 81.40\% \\
            Precision & 84.45\% \\
            Recall & 81.40\% \\
            F1-Score & 82.21\% \\
            \hline
        \end{tabular}
        \caption{Evaluation metrics for the decision tree classifier on the test set for $\textit{prefix\_length}=11$.}
        \label{tab:evaluation_metrics_11}
    \end{minipage}
\end{table}

\subsection{Recommendation Analysis}

\paragraph{Recommendation Generation}
To illustrate recommendation extraction, a simplified decision tree is considered (Figure~\ref{fig:decision_tree_recommendation}).
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../media/decision_tree_small.png}
    \caption{Simplified decision tree used to illustrate the recommendation extraction process.
    Given a node, the left child branch is taken if the condition is true (activity is absent in the trace), while the right child branch is taken if the condition is false (activity is present in the trace).}
    \label{fig:decision_tree_recommendation}
\end{figure}
Applying the extraction of positive paths (Algorithm~\ref{alg:get_positive_paths}) to this tree, we obtain the following positive paths with their confidence scores:
\begin{itemize}
    \item Path 1: \textit{Turning Q.C.} = F $\wedge$ \textit{Grinding Rework - Machine 27} = F $\wedge$ \textit{Turning - Machine 9} = F $\wedge$ \textit{Turning \& Milling Q.C.} = F with confidence $0.77$;
    \item Path 2: \textit{Turning Q.C.} = F $\wedge$ \textit{Grinding Rework - Machine 27} = F $\wedge$ \textit{Turning - Machine 9} = T with confidence $0.80$;
    \item Path 3: \textit{Turning Q.C.} = T $\wedge$ \textit{Turning \& Milling - Machine 10} = F $\wedge$ \textit{Final Inspection Q.C.} = F $\wedge$ \textit{Packing} = T with confidence $1.0$;
    \item Path 4: \textit{Turning Q.C.} = T $\wedge$ \textit{Turning \& Milling - Machine 10} = T with confidence $1.0$;
\end{itemize}
Given a trace prefix $\sigma = \{\textit{Final Inspection Q.C.}=T, \textit{Packing}=T, \textit{Grinding Rework - Machine 27}=T\}$ predicted as negative, compliant paths are identified as follows:
Given a trace prefix $\sigma = \{\textit{Final Inspection Q.C.}=T, \textit{Packing}=T, \textit{Grinding Rework - Machine 27}=T\}$ predicted as negative, compliant paths are identified as follows:
\begin{itemize}
    \item Paths 1 and 2 are non-compliant, violating \textit{Grinding Rework - Machine 27}=F.
    \item Path 3 is compliant.
    \item Path 4 is non-compliant, violating \textit{Final Inspection Q.C.}=F.
\end{itemize}
Given this, the recommendations for the given prefix trace are the activities that need to be added to follow Path 3.
Since both \textit{Turning Q.C.} and \textit{Turning \& Milling - Machine 10} need to be true to follow Path 3, the recommended activities will be those.
This is illustrated in Figure~\ref{fig:recommendation_tree}, with the green dotted nodes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../media/recommendation_tree.png}
    \caption{The blue squared nodes represent the activities present in the given prefix trace. The green highlighted nodes represent the recommended activities.}
    \label{fig:recommendation_tree}
\end{figure}

\paragraph{Recommendations Quality}
Tables~\ref{tab:evaluation_metrics_rec_5} and~\ref{tab:evaluation_metrics_rec_11} present the evaluation of our recommendation system.
Metrics are approximations, as the ground truth is based on actual traces rather than hypothetical scenarios in which recommendations were followed.
\\\\
For $\textit{prefix\_length}=5$, all metrics are $0\%$, indicating that the few generated recommendations were not followed.
This is likely due to insufficient contextual information in very short prefixes, making it difficult for the system to provide effective suggestions.
\\\\For $\textit{prefix\_length}=11$, the recommendation system demonstrates substantial improvement across all metrics compared to shorter prefix lengths.
This demonstrates that a sufficient prefix length is required for the recommendation system to reliably identify actionable patterns and align them with successful outcomes.
\begin{table}[H]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Metric} & \textbf{Value} \\
            \hline
            Accuracy & 0.00\% \\
            Precision & 0.00\% \\
            Recall & 0.00\% \\
            F1-Score & 0.00\% \\
            \hline
        \end{tabular}
        \caption{Evaluation metrics for the recommendation system on the test set for $\textit{prefix\_length}=5$.}
        \label{tab:evaluation_metrics_rec_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Metric} & \textbf{Value} \\
            \hline
            Accuracy & 46.67\% \\
            Precision & 60.00\% \\
            Recall & 33.33\% \\
            F1-Score & 42.86\% \\
            \hline
        \end{tabular}
        \caption{Evaluation metrics for the recommendation system on the test set for $\textit{prefix\_length}=11$.}
        \label{tab:evaluation_metrics_rec_11}
    \end{minipage}
\end{table}

Figures~\ref{fig:rec_confusion_matrix_5} and~\ref{fig:rec_confusion_matrix_11} present the corresponding confusion matrices, providing further insight into recommendation outcomes.
\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../media/recommendation_confusion_matrix_5.png}
        \caption{Confusion matrix of recommendations ($\textit{prefix\_length}=5$).}
        \label{fig:rec_confusion_matrix_5}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../media/recommendation_confusion_matrix_11.png}
        \caption{Confusion matrix of recommendations ($\textit{prefix\_length}=11$).}
        \label{fig:rec_confusion_matrix_11}
    \end{minipage}
\end{figure}

\subsection{Performance by Prefix Length}
Figure~\ref{fig:prefix_length_f1_statistics} compares the F1-score for the decision tree classifier and the recommendation system across different prefix lengths.
The decision tree classifier maintains relatively stable performance, whereas the recommendation system exhibits marked improvement as prefix length increases.
This trend highlights that, while outcome prediction can remain effective with short prefixes, meaningful recommendation generation requires a minimum threshold of contextual information.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../media/prefix_length_f1_statistics.png}
    \caption{The box plots illustrate the distribution of F1-scores across different prefix lengths for both the decision tree classifier and the recommendation system.}
    \label{fig:prefix_length_f1_statistics}
\end{figure}